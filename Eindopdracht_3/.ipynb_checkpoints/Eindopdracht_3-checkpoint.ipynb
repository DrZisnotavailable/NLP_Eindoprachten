{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66594831-442a-45ba-b4d3-fa2d0d6e3fe7",
   "metadata": {},
   "source": [
    "# Character-level LSTM met Keras — Iliad\n",
    "\n",
    "**Doel:** een char-level LSTM trainen op `iliad.txt` (lowercased, verder raw), met:\n",
    "- sliding window van lengte **m = 100** karakters → voorspellen **volgend** karakter\n",
    "- model: LSTM(256, return_sequences=True) → Dropout(0.2) → LSTM(256) → Dropout(0.1) → Dense(|V|, softmax)\n",
    "- optimizer **Adam**, loss **categorical_crossentropy**\n",
    "- **ModelCheckpoint** op `loss` (save_best_only)\n",
    "- na trainen: 1000 karakters genereren uit een random startsequence\n",
    "\n",
    "**Waarom char-level (i.p.v. word-level)?**\n",
    "- Geen tokenisatie/cleaning nodig; model leert spelling/patronen direct uit ruwe karakters.\n",
    "- Nadeel: langere sequenties nodig om zinsstructuur te vatten en vaak trager leren.\n",
    "\n",
    "**Wat ik ga loggen/kiezen:**\n",
    "- X vorm: `(n_sequences, 100, 1)` (1 feature = integer per timestep)\n",
    "- X normalisatie: delen door vocab_size (zodat input ≈ [0,1])\n",
    "- y: one-hot met `to_categorical`\n",
    "- 20 epochs, batch_size 128 (of minder als resources krap zijn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e605e6f-3f22-4399-b1ef-2c244e43ed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tekstlengte (chars): 1,116,792\n",
      "Voorbeeld (eerste 200 tekens):\n",
      "the project gutenberg ebook of the iliad\n",
      "    \n",
      "this ebook is for the use of anyone anywhere in the united states and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever.\n"
     ]
    }
   ],
   "source": [
    "# STAP 1: LAAD ILIAD, RAW CHAR DATA, LOWERCASE\n",
    "# ---------------------------------------------\n",
    "# Opdracht: \"Laad de data uit iliad.txt ... en converteer naar lower case.\n",
    "# Geen verdere cleaning; we gebruiken ruwe character data.\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Reproduceerbaarheid (voor zover mogelijk met GPU/cuDNN)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Pad naar databestand\n",
    "ILIAD_PATH = \"iliad.txt\"\n",
    "\n",
    "# Sliding-window lengte m (volgens opdracht = 100)\n",
    "SEQ_LEN = 100\n",
    "\n",
    "# Aantal te genereren karakters\n",
    "GEN_STEPS = 1000\n",
    "\n",
    "# Checkpoint-pad (beste loss)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "BEST_CKPT_PATH = \"checkpoints/lstm_char_best.h5\"\n",
    "\n",
    "# 1) Laad en lowercase\n",
    "with open(ILIAD_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    raw_text = f.read().lower()\n",
    "\n",
    "print(f\"Tekstlengte (chars): {len(raw_text):,}\")\n",
    "print(\"Voorbeeld (eerste 200 tekens):\")\n",
    "print(raw_text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd6f530-a5f5-4d60-bc29-f17aeced5126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 146\n",
      "Chars (eerste 100): ['\\n', ' ', '!', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '§', 'à', 'ä', 'æ', 'è', 'é', 'ê', 'ë', 'ï', 'ò', 'ô', 'ö', 'ù', 'ü', 'œ', 'ά', 'έ', 'ή', 'ί', 'α', 'β', 'γ', 'δ', 'ε', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'ς', 'σ', 'τ', 'υ', 'φ', 'χ', 'ω', 'ό']\n",
      "Aantal sequences (n): 1,116,692 | Sequence length (m): 100\n",
      "Voorbeeld X_int[0][:20]: [50 38 35  1 46 48 45 40 35 33 50  1 37 51 50 35 44 32 35 48] -> Y_int[0]: 39 ('i')\n"
     ]
    }
   ],
   "source": [
    "# STAP 2: UNIEKE KARAKTERS, MAPPINGS, SLIDING WINDOW\n",
    "# ---------------------------------------------------\n",
    "# - Maak lijst unieke chars en dicts: char_to_int, int_to_char\n",
    "# - Sliding window (lengte 100): X = sequences van ints, Y = 'volgend' char (als int)\n",
    "\n",
    "# Unieke karakters\n",
    "chars = sorted(list(set(raw_text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Chars (eerste 100):\", chars[:100])\n",
    "\n",
    "# Mappings\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for c, i in char_to_int.items()}\n",
    "\n",
    "# Sliding window\n",
    "X_int = []  # elke entry is lijst met 100 ints\n",
    "Y_int = []  # target: int van het karakter na het window\n",
    "\n",
    "for i in range(0, len(raw_text) - SEQ_LEN):\n",
    "    seq_in = raw_text[i : i + SEQ_LEN]\n",
    "    seq_out = raw_text[i + SEQ_LEN]  # het eerstvolgende karakter\n",
    "    X_int.append([char_to_int[ch] for ch in seq_in])\n",
    "    Y_int.append(char_to_int[seq_out])\n",
    "\n",
    "X_int = np.array(X_int, dtype=np.int32)\n",
    "Y_int = np.array(Y_int, dtype=np.int32)\n",
    "\n",
    "print(f\"Aantal sequences (n): {X_int.shape[0]:,} | Sequence length (m): {SEQ_LEN}\")\n",
    "print(\"Voorbeeld X_int[0][:20]:\", X_int[0][:20], \"-> Y_int[0]:\", Y_int[0], f\"('{int_to_char[Y_int[0]]}')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82121338-4806-4589-bba7-670971b2ce5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dtype: int32\n",
      "X shape: (1116692, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# STAP 3: RESHAPE NAAR 3D TENSOR (n, m, 1)\n",
    "# ----------------------------------------\n",
    "# Opdracht: \"Reshape de data in X naar (n, m, 1) ... 1 is aantal features, integer-data.\"\n",
    "# Let op: normaliseren doen we pas in stap 4.\n",
    "\n",
    "n_sequences = X_int.shape[0]        # n\n",
    "m = SEQ_LEN                         # m (100)\n",
    "X = X_int.reshape((n_sequences, m, 1))  # (n, m, 1)\n",
    "\n",
    "print(\"X dtype:\", X.dtype)\n",
    "print(\"X shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "352a916e-3d66-4010-afd2-ef436ea3885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X na normalisatie — dtype: float32 min: 0.0 max: 0.9931507\n"
     ]
    }
   ],
   "source": [
    "# STAP 4: NORMALISEREN\n",
    "# --------------------\n",
    "# \"Normaliseer X door te delen door het aantal karakters in de dictionary.\"\n",
    "# Hierdoor worden integer indices geschaald naar [0,1], wat training stabiliseert.\n",
    "\n",
    "X = X.astype(np.float32) / float(vocab_size)\n",
    "print(\"X na normalisatie — dtype:\", X.dtype, \"min:\", X.min(), \"max:\", X.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27556b83-b9c5-4db7-866a-70cfdeb72df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (1116692, 146) (verwacht: (n, vocab_size))\n"
     ]
    }
   ],
   "source": [
    "# STAP 5: ONE-HOT ENCODING VAN Y\n",
    "# ------------------------------\n",
    "# \"Converteer y naar een one-hot encoded representatie.\"\n",
    "# Outputdimensie = vocab_size (|V|), dus to_categorical(..., num_classes=vocab_size).\n",
    "\n",
    "y = to_categorical(Y_int, num_classes=vocab_size).astype(np.float32)\n",
    "print(\"y shape:\", y.shape, \"(verwacht: (n, vocab_size))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20db274b-86e6-45a8-bbe4-254e7715c844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sietse\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">264,192</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">146</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">37,522</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m264,192\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m525,312\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m146\u001b[0m)                 │          \u001b[38;5;34m37,522\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">827,026</span> (3.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m827,026\u001b[0m (3.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">827,026</span> (3.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m827,026\u001b[0m (3.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STAP 6: MODELDEFINITIE (SEQUENTIAL)\n",
    "# -----------------------------------\n",
    "# Lagen (exact volgens opdracht):\n",
    "# 1) LSTM(256, input_shape=(m,1), return_sequences=True)\n",
    "# 2) Dropout(0.2)\n",
    "# 3) LSTM(256)\n",
    "# 4) Dropout(0.1)\n",
    "# 5) Dense(vocab_size, activation='softmax')\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(m, 1), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(256),\n",
    "    Dropout(0.1),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a147b59-4b9d-40d6-aac9-fa7c37501567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gecompileerd (optimizer=adam, loss=categorical_crossentropy).\n"
     ]
    }
   ],
   "source": [
    "# STAP 7: COMPILE\n",
    "# ---------------\n",
    "# \"Compile met de adam optimizer en categorical_crossentropy als loss-functie.\"\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "print(\"Model gecompileerd (optimizer=adam, loss=categorical_crossentropy).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e3c36-4981-483e-ac3c-faa623d2b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m 125/8725\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00:32\u001b[0m 841ms/step - loss: 3.4813"
     ]
    }
   ],
   "source": [
    "# STAP 8: TRAINING + CHECKPOINT\n",
    "# -----------------------------\n",
    "# \"Fit het model ... 20 epochs, batch_size 128. Gebruik ModelCheckpoint(..., monitor='loss',\n",
    "#  save_best_only=True, mode='min').\"\n",
    "#\n",
    "# NB: Dit kan op CPU lang duren. Met GPU (Colab) gaat het per epoch sneller.\n",
    "# We monitoren 'loss' (train loss), zoals expliciet gevraagd.\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=BEST_CKPT_PATH,\n",
    "    monitor='loss',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint_cb]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb0231-3b7b-4f74-9080-541334d10f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAP 9: BESTE MODEL LADEN + RANDOM STARTSEQUENCE\n",
    "# ------------------------------------------------\n",
    "# \"Laad het model met de beste loss en compile dit. Kies een random startsequence uit X en print deze.\"\n",
    "\n",
    "best_model = load_model(BEST_CKPT_PATH)\n",
    "best_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "print(\"Beste model geladen uit:\", BEST_CKPT_PATH)\n",
    "\n",
    "# Kies een random seedsequence uit de trainingsdata (als ints, lengte 100)\n",
    "start_idx = np.random.randint(0, n_sequences)\n",
    "seed_seq = X_int[start_idx].tolist()  # let op: X_int zijn *ints* (pre-normalisatie)\n",
    "seed_text = \"\".join(int_to_char[i] for i in seed_seq)\n",
    "\n",
    "print(\"\\n=== Startsequence (100 chars) ===\")\n",
    "print(seed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57dcedb-1981-43d8-bfa5-c82e236f1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAP 10: GENEREREN VAN 1000 KARAKTERS\n",
    "# -------------------------------------\n",
    "# Voor elke stap:\n",
    "# - Reshape huidige sequence naar (1, m, 1) en normaliseer (/ vocab_size)\n",
    "# - Predict softmax, neem argmax → integer\n",
    "# - Converteer int → char en print zonder newline\n",
    "# - Schuif window: laatste 99 + nieuw char\n",
    "\n",
    "generated = []\n",
    "pattern = list(seed_seq)  # kopie van de startsequence (ints)\n",
    "\n",
    "for step in range(GEN_STEPS):\n",
    "    x_in = np.array(pattern, dtype=np.float32).reshape(1, m, 1) / float(vocab_size)\n",
    "    proba = best_model.predict(x_in, verbose=0)[0]  # (vocab_size,)\n",
    "    next_idx = int(np.argmax(proba))                # argmax volgens opdracht\n",
    "    next_char = int_to_char[next_idx]\n",
    "    print(next_char, end=\"\")                        # geen newline\n",
    "    generated.append(next_char)\n",
    "    pattern = pattern[1:] + [next_idx]             # schuif window 1 naar rechts\n",
    "\n",
    "print(\"\\n\\n=== Klaar met genereren ===\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d73e55-e66e-4a2d-80aa-8353f17a4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Observaties, tekortkomingen, verbeteringen\n",
    "\n",
    "**Wat valt op in de gegenereerde tekst?**\n",
    "- Vaak zie je dat het model **lokale spelling** en **veelvoorkomende n-grams** leert (bijv. \"the \", \"and \").\n",
    "- Globale coherentie (lange-afstand afhankelijkheden over vele zinnen/regels) is **beperkt**:\n",
    "  het model vergeet context over langere lengtes en \"zwerft\" soms semantisch.\n",
    "\n",
    "**Tekortkomingen**\n",
    "- **Char-level** leert traag en heeft veel data nodig; syntaxis/semantiek op paragraafniveau is lastig.\n",
    "- We trainen alleen op **training loss** (geen val loss), risico op **overfitting**.\n",
    "- Keuze **argmax** maakt de tekst deterministisch en soms saai / snel vastlopen in repetitie.\n",
    "\n",
    "**Verbeteringen**\n",
    "1. **Temperature sampling** of **top-k / nucleus sampling** i.p.v. argmax → meer variatie, minder loops.\n",
    "2. **Validation split + EarlyStopping** → betere generalisatie.\n",
    "3. **Groter model / meer lagen** of **bidirectionele LSTM** (voor training; generation blijft uni-directioneel).\n",
    "4. **GRU/Transformer**: modernere architectuur kan langere contexten beter aan.\n",
    "5. **Meer data** (bijv. Iliad + Odyssey samen) of **data augmentation** (case-mix? niet verplicht).\n",
    "6. **Sequence length** en **overlap** tunen (langer window kan helpen, maar kost meer geheugen/tijd).\n",
    "7. **Learning rate schedulers** of **gradient clipping** → stabielere training.\n",
    "8. **Checkpoint op val_loss** als je een valset maakt, i.p.v. training loss.\n",
    "\n",
    "> NB: Resource hint — trainen op CPU kan lang duren. Met GPU (b.v. Colab) gaat dit per epoch een stuk sneller.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
