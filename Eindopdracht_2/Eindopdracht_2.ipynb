{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a3da38-e297-46d4-9071-e46cd681bdaf",
   "metadata": {},
   "source": [
    "# Eindopdracht 2: Machine Translation & Document Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd10e14-1e3e-4a2b-bb6a-6b5c6ba1c8f6",
   "metadata": {},
   "source": [
    "Naam: Sietse Neve\n",
    "\n",
    "Studentnummer: 1810364"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc13d2-242f-4212-bf7f-072023bcd7fd",
   "metadata": {},
   "source": [
    "# Naïeve Machine Translation (EN → FR) met Vector Embeddings\n",
    "\n",
    "Dit notebook bouwt een *simpel maar compleet* woord-naar-woord-vertalsysteem:\n",
    "- We leren een **lineaire mapping** \\(R\\) tussen Engelse en Franse embed-ruimtes\n",
    "  door de **Frobenius-norm**-loss te minimaliseren met **gradient descent**:\n",
    "  \n",
    "  \\[\n",
    "  L(R) = \\frac{1}{m}\\lVert X R - Y \\rVert_F^2\n",
    "  \\quad\\Rightarrow\\quad\n",
    "  \\nabla_R L = \\frac{2}{m} X^\\top (X R - Y)\n",
    "  \\]\n",
    "  \n",
    "- Nieuwe woorden vertalen we door:\n",
    "  1) embedding opzoeken (EN)  \n",
    "  2) projecteren met \\(R\\) naar FR-ruimte  \n",
    "  3) **k-nearest neighbors** zoeken o.b.v. **cosine similarity** in FR-ruimte\n",
    "\n",
    "Met de settings uit de opdracht (400 stappen, lr=0.8) kun je doorgaans >55% top-1 accuracy halen op de testset.\n",
    "\n",
    "**Bestanden die je nodig hebt** (zoals in de opdracht):\n",
    "- `en_embeddings.p` — Engelse embeddings (subset)\n",
    "- `fr_embeddings.p` — Franse embeddings (subset)\n",
    "- `en_fr.train.txt` — train-woordparen (EN FR)\n",
    "- `en_fr.test.txt` — test-woordparen (EN FR)\n",
    "\n",
    "We houden de code modulair en rijk becommentarieerd, vergelijkbaar met de stijl van je eerdere uitwerking — maar alles hier is fris geschreven voor deze opdracht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36270f73-752c-458c-aeb6-948bc88a3c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f136c2-f336-4943-b640-f5154c020fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Laadt een pickled dict: { \"woord\": np.ndarray(d,), ... }.\n",
    "    - Converteert waarden naar float32 voor snelheid/zuinig geheugen.\n",
    "    - Geen ordeningsaanname; voor k-NN bouwen we later een matrix.\n",
    "\n",
    "    Waarom pickle?\n",
    "    - Snel en direct voor Python dicts met numpy arrays (zoals deze cursusbestanden).\n",
    "    \"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    for k, v in list(data.items()):\n",
    "        data[k] = np.asarray(v, dtype=np.float32).reshape(-1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_pairs(path: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Leest (en, fr)-paren uit een 2-koloms textbestand met spatie-separatie.\n",
    "    Lege regels worden overgeslagen.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            en, fr = line.split()\n",
    "            pairs.append((en, fr))\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8a9fa8-3bca-425b-948e-5164e34daacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alignment_matrices(\n",
    "    pairs: Sequence[Tuple[str, str]],\n",
    "    en_embeddings: Dict[str, np.ndarray],\n",
    "    fr_embeddings: Dict[str, np.ndarray],\n",
    ") -> Tuple[np.ndarray, np.ndarray, List[Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    Maakt X (EN) en Y (FR) door voor elk (en, fr)-paar de bijbehorende embeddings te stapelen.\n",
    "    - Pairs met ontbrekende embeddings worden gefilterd (zoals gebruikelijk in de opdracht).\n",
    "    - Controleert bovendien de embed-dimensies (moeten gelijk zijn aan beide kanten).\n",
    "\n",
    "    Returns:\n",
    "        X: (m, d), Y: (m, d), kept_pairs: de effectief gebruikte paren\n",
    "    \"\"\"\n",
    "    X_rows, Y_rows, kept = [], [], []\n",
    "    for en, fr in pairs:\n",
    "        v_en = en_embeddings.get(en)\n",
    "        v_fr = fr_embeddings.get(fr)\n",
    "        if v_en is None or v_fr is None:\n",
    "            continue\n",
    "        v_en = np.asarray(v_en, dtype=np.float32).reshape(-1)\n",
    "        v_fr = np.asarray(v_fr, dtype=np.float32).reshape(-1)\n",
    "        if v_en.shape != v_fr.shape:\n",
    "            # Embedding-dimensie mismatch; skip dit paar om bugs te voorkomen\n",
    "            continue\n",
    "        X_rows.append(v_en)\n",
    "        Y_rows.append(v_fr)\n",
    "        kept.append((en, fr))\n",
    "\n",
    "    if not X_rows:\n",
    "        raise ValueError(\"Geen bruikbare trainingpairs over. Controleer je bestanden/paths.\")\n",
    "    X = np.vstack(X_rows).astype(np.float32)\n",
    "    Y = np.vstack(Y_rows).astype(np.float32)\n",
    "    return X, Y, kept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499f4352-0baf-4953-b230-b92c895d0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GDConfig:\n",
    "    \"\"\"\n",
    "    Config voor full-batch gradient descent.\n",
    "    - num_iters & learning_rate volgen de opdracht.\n",
    "    - l2_reg is optioneel (default 0.0 om aan te sluiten bij de basisopdracht).\n",
    "    - init kan \"zeros\" (zoals vaak in colleges) of \"eye\" (kan soms sneller convergeren).\n",
    "    \"\"\"\n",
    "    num_iters: int = 400\n",
    "    learning_rate: float = 0.8\n",
    "    l2_reg: float = 0.0\n",
    "    init: str = \"zeros\"  # of \"eye\"\n",
    "\n",
    "\n",
    "def _loss_and_grad(X: np.ndarray, Y: np.ndarray, R: np.ndarray, l2_reg: float = 0.0) -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Berekent:\n",
    "        L(R) = (1/m)||XR - Y||_F^2 + λ||R||_F^2\n",
    "    en de gradiënt:\n",
    "        ∇_R L = (2/m) Xᵀ (X R - Y) + 2λR\n",
    "\n",
    "    Waarom Frobenius?\n",
    "    - Dit is precies de som van kwadraten over alle elementen; elegant en efficiënt te differentiëren.\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    XR_minus_Y = X @ R - Y\n",
    "    data_loss = (XR_minus_Y ** 2).sum() / m\n",
    "    reg_loss = l2_reg * (R ** 2).sum()\n",
    "    loss = float(data_loss + reg_loss)\n",
    "\n",
    "    grad = (2.0 / m) * (X.T @ XR_minus_Y)\n",
    "    if l2_reg:\n",
    "        grad = grad + 2.0 * l2_reg * R\n",
    "    return loss, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01019049-95f7-421f-8e8a-92067429b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_map(\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    cfg: GDConfig = GDConfig(),\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"\n",
    "    Leert R (d×d) met full-batch GD op de Frobenius-loss.\n",
    "    - Duidelijke logging per ~10% van het aantal iteraties.\n",
    "    - Retourneert R en de loss-geschiedenis (voor diagnose/plots).\n",
    "    \"\"\"\n",
    "    m, d = X.shape\n",
    "    if Y.shape != (m, d):\n",
    "        raise ValueError(f\"Shape mismatch: X{X.shape} vs Y{Y.shape}\")\n",
    "\n",
    "    if cfg.init == \"zeros\":\n",
    "        R = np.zeros((d, d), dtype=np.float32)\n",
    "    elif cfg.init == \"eye\":\n",
    "        R = np.eye(d, dtype=np.float32)\n",
    "    else:\n",
    "        raise ValueError(\"cfg.init moet 'zeros' of 'eye' zijn.\")\n",
    "\n",
    "    losses = []\n",
    "    for it in range(1, cfg.num_iters + 1):\n",
    "        loss, grad = _loss_and_grad(X, Y, R, l2_reg=cfg.l2_reg)\n",
    "        R = R - cfg.learning_rate * grad\n",
    "        losses.append(loss)\n",
    "\n",
    "        if verbose and (it == 1 or it % max(1, cfg.num_iters // 10) == 0 or it == cfg.num_iters):\n",
    "            print(f\"[GD] iter {it:4d}/{cfg.num_iters} | loss = {loss:.6f}\")\n",
    "    return R, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a973c0-00af-42e6-a339-3252697cec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _l2_normalize_rows(M: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    L2-normaliseert elke rij afzonderlijk. Dit maakt cosine-similarity sneller/robuuster.\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(M, axis=1, keepdims=True)\n",
    "    norms = np.maximum(norms, eps)\n",
    "    return M / norms\n",
    "\n",
    "\n",
    "def build_french_matrix(fr_embeddings: Dict[str, np.ndarray], l2_normalize: bool = True) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Pakt FR-embeddings in een (n_fr, d)-matrix + een woordenlijst in dezelfde volgorde.\n",
    "    (Optioneel) normaliseert de rijen één keer vooraf (performant).\n",
    "    \"\"\"\n",
    "    fr_words = list(fr_embeddings.keys())\n",
    "    if not fr_words:\n",
    "        raise ValueError(\"Franse embedding-dict is leeg.\")\n",
    "\n",
    "    d = fr_embeddings[fr_words[0]].shape[0]\n",
    "    F = np.vstack([fr_embeddings[w].reshape(1, -1) for w in fr_words]).astype(np.float32)\n",
    "    if l2_normalize:\n",
    "        F = _l2_normalize_rows(F)\n",
    "    return F, fr_words\n",
    "\n",
    "\n",
    "def _cosine_scores(vec: np.ndarray, M_unitrows: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cosine-similarity tussen een *enkele* vector en alle rijen van M.\n",
    "    Aannames:\n",
    "    - Rijen van M zijn al L2-genormaliseerd.\n",
    "    - We normaliseren `vec` ook één keer.\n",
    "    \"\"\"\n",
    "    v = vec.astype(np.float32).reshape(1, -1)\n",
    "    v = _l2_normalize_rows(v)\n",
    "    return (v @ M_unitrows.T).ravel()\n",
    "\n",
    "\n",
    "def translate(\n",
    "    english_word: str,\n",
    "    R: np.ndarray,\n",
    "    en_embeddings: Dict[str, np.ndarray],\n",
    "    F_unitrows: np.ndarray,\n",
    "    fr_words: Sequence[str],\n",
    "    k: int = 1,\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    EN-woord → projecteer met R → top-k dichtste FR-woorden o.b.v. cosine.\n",
    "    Retourneert lijst van (fr_woord, score), gesorteerd van best naar slechtst.\n",
    "    \"\"\"\n",
    "    v_en = en_embeddings.get(english_word)\n",
    "    if v_en is None:\n",
    "        raise KeyError(f\"'{english_word}' niet gevonden in EN-embeddings.\")\n",
    "\n",
    "    v_hat = (np.asarray(v_en, dtype=np.float32) @ R).astype(np.float32)  # projectie naar FR-ruimte\n",
    "    scores = _cosine_scores(v_hat, F_unitrows)\n",
    "    topk = np.argsort(-scores)[:k]\n",
    "    return [(fr_words[i], float(scores[i])) for i in topk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea51990f-4496-473d-a1d2-b60cd3c87b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_gold_map(pairs: Sequence[Tuple[str, str]]) -> Dict[str, set]:\n",
    "    \"\"\"\n",
    "    Zet (en, fr)-paren om naar: en_woord -> {mogelijke_fr_vertalingen}.\n",
    "    Zo telt een voorspelling ook als correct als er meerdere correcte FR-varianten zijn.\n",
    "    \"\"\"\n",
    "    gold = {}\n",
    "    for en, fr in pairs:\n",
    "        gold.setdefault(en, set()).add(fr)\n",
    "    return gold\n",
    "\n",
    "\n",
    "def evaluate_accuracy(\n",
    "    test_pairs: Sequence[Tuple[str, str]],\n",
    "    R: np.ndarray,\n",
    "    en_embeddings: Dict[str, np.ndarray],\n",
    "    F_unitrows: np.ndarray,\n",
    "    fr_words: Sequence[str],\n",
    "    top_k: int = 1,\n",
    "    quiet: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Berekent top-k accuracy op de testset.\n",
    "    - Skipt EN-woorden zonder embedding (we kunnen niet voorspellen).\n",
    "    - Correct als *één* van de top-k FR-kandidaten in de goud-set zit.\n",
    "    \"\"\"\n",
    "    gold = _build_gold_map(test_pairs)\n",
    "\n",
    "    total, correct = 0, 0\n",
    "    for en_word, valid_fr in gold.items():\n",
    "        if en_word not in en_embeddings:\n",
    "            continue\n",
    "        total += 1\n",
    "        preds = translate(en_word, R, en_embeddings, F_unitrows, fr_words, k=top_k)\n",
    "        pred_words = {w for (w, _) in preds}\n",
    "        if pred_words & valid_fr:\n",
    "            correct += 1\n",
    "\n",
    "    if total == 0:\n",
    "        raise ValueError(\"Geen evalueerbare testitems (EN-woorden niet in embeddings?).\")\n",
    "    acc = correct / total\n",
    "    if not quiet:\n",
    "        print(f\"Evaluated on {total} woorden — correct: {correct} — Acc@{top_k} = {acc:.2%}\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27b768b0-3e61-44ff-be00-25463c8402cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,370 EN-embeddings geladen\n",
      "5,766 FR-embeddings geladen\n",
      "10,872 train-paren | 2,943 test-paren\n"
     ]
    }
   ],
   "source": [
    "EN_EMB_PATH = \"en_embeddings.p\"\n",
    "FR_EMB_PATH = \"fr_embeddings.p\"\n",
    "TRAIN_PATH  = \"en_fr.train.txt\"\n",
    "TEST_PATH   = \"en_fr.test.txt\"\n",
    "\n",
    "# Load data\n",
    "en_emb = load_embeddings(EN_EMB_PATH)\n",
    "fr_emb = load_embeddings(FR_EMB_PATH)\n",
    "train_pairs = load_pairs(TRAIN_PATH)\n",
    "test_pairs  = load_pairs(TEST_PATH)\n",
    "\n",
    "print(f\"{len(en_emb):,} EN-embeddings geladen\")\n",
    "print(f\"{len(fr_emb):,} FR-embeddings geladen\")\n",
    "print(f\"{len(train_pairs):,} train-paren | {len(test_pairs):,} test-paren\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef797e2-3bac-4d20-bdf7-2e1c95d7c04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-paren gebruikt na filtering: 5,717 (X: (5717, 300), Y: (5717, 300))\n",
      "[GD] iter    1/400 | loss = 1.000000\n",
      "[GD] iter   40/400 | loss = 0.579866\n",
      "[GD] iter   80/400 | loss = 0.570384\n",
      "[GD] iter  120/400 | loss = 0.568682\n",
      "[GD] iter  160/400 | loss = 0.568221\n",
      "[GD] iter  200/400 | loss = 0.568064\n",
      "[GD] iter  240/400 | loss = 0.568004\n",
      "[GD] iter  280/400 | loss = 0.567977\n",
      "[GD] iter  320/400 | loss = 0.567964\n",
      "[GD] iter  360/400 | loss = 0.567958\n",
      "[GD] iter  400/400 | loss = 0.567955\n"
     ]
    }
   ],
   "source": [
    "# Align training-matrices\n",
    "X, Y, kept_train_pairs = build_alignment_matrices(train_pairs, en_emb, fr_emb)\n",
    "print(f\"Train-paren gebruikt na filtering: {len(kept_train_pairs):,} (X: {X.shape}, Y: {Y.shape})\")\n",
    "\n",
    "# GD-config zoals in de opdracht (je mag experimenteren)\n",
    "cfg = GDConfig(num_iters=400, learning_rate=0.8, l2_reg=0.0, init=\"zeros\")\n",
    "\n",
    "# Train\n",
    "R, losses = fit_linear_map(X, Y, cfg=cfg, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe3d11b4-fa53-415b-b288-386905406848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot de loss-curve ter controle (vereist matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"GD training loss\")\n",
    "plt.xlabel(\"Iteratie\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e146703-194e-45ae-b0f2-dd7909ebd664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR-matrix shape: (5766, 300), aantal FR-woorden: 5,766\n"
     ]
    }
   ],
   "source": [
    "F_unit, fr_words = build_french_matrix(fr_emb, l2_normalize=True)\n",
    "print(f\"FR-matrix shape: {F_unit.shape}, aantal FR-woorden: {len(fr_words):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bd28748-40c6-4f5b-8883-fc92e66c656a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 1438 woorden — correct: 666 — Acc@1 = 46.31%\n",
      "Evaluated on 1438 woorden — correct: 919 — Acc@5 = 63.91%\n",
      "Evaluated on 1438 woorden — correct: 1007 — Acc@10 = 70.03%\n",
      "Evaluated on 1438 woorden — correct: 1058 — Acc@15 = 73.57%\n",
      "Top-1 accuracy: 46.31%\n",
      "Top-5 accuracy: 63.91%\n",
      "Top-10 accuracy: 70.03%\n",
      "Top-15 accuracy: 73.57%\n"
     ]
    }
   ],
   "source": [
    "acc1 = evaluate_accuracy(test_pairs, R, en_emb, F_unit, fr_words, top_k=1)\n",
    "acc5 = evaluate_accuracy(test_pairs, R, en_emb, F_unit, fr_words, top_k=5)\n",
    "acc10 = evaluate_accuracy(test_pairs, R, en_emb, F_unit, fr_words, top_k=10)\n",
    "acc15 = evaluate_accuracy(test_pairs, R, en_emb, F_unit, fr_words, top_k=15)\n",
    "\n",
    "print(f\"Top-1 accuracy: {acc1:.2%}\")\n",
    "print(f\"Top-5 accuracy: {acc5:.2%}\")\n",
    "print(f\"Top-10 accuracy: {acc10:.2%}\")\n",
    "print(f\"Top-15 accuracy: {acc15:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0632e7e8-3674-4670-b949-f52ab1782817",
   "metadata": {},
   "source": [
    "# Document Search op basis van Word Embeddings + LSH (Approximate KNN)\n",
    "\n",
    "Doel: Gegeven een *querytekst* de *n* meest gelijkende zinnen (documenten) uit een corpus (bijv. Ilias/Odyssee) terugvinden.\n",
    "\n",
    "Benadering:\n",
    "1. **Cleaning & tokenization**: we normaliseren tekst zodat embeddings beter matchen.\n",
    "2. **Documentvector**: som (of gemiddelde) van woord-embeddings van een zin → één vector per document.\n",
    "3. **LSH (Locality Sensitive Hashing) met random hyperplanes**:\n",
    "   - Projecteer vectors op *L* willekeurige hypervlakken.\n",
    "   - Neem het teken (±) per projectie → *L*-bits handtekening → bucket-ID.\n",
    "   - Alleen de *kandidaten* in dezelfde bucket als de query bekijken (≈ 1 bucket).\n",
    "4. **k-NN met cosine similarity** *binnen* de gevonden bucket → top-*n* meest gelijkende zinnen.\n",
    "\n",
    "Waarom LSH?\n",
    "- Bespaart zoektijd: in plaats van *alle* documenten te vergelijken, beperken we ons tot één (kleine) bucket.\n",
    "- Nadeel: door toeval kun je matches missen → daarom \"Approximate\" KNN.\n",
    "\n",
    "Benodigd:\n",
    "- Een embeddings-dictionary: `{woord -> np.ndarray(d,)}` (zelfde formaat als Deel I).\n",
    "- Een lijst zinnen (`sentences`) uit je Ilias/Odyssee-voorbewerking.\n",
    "- Een cleaning-functie uit eerdere opdrachten (hier leveren we een nette, generieke variant).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60b28d52-25c1-42a4-a15e-cbf6faeeb6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Sequence, Iterable, Optional\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "858ed859-9713-4076-8d6e-2db785ee428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: Pas deze cleaning zo nodig aan jouw eerdere opdracht (zelfde principes).\n",
    "# Hier een solide, generieke variant: lowercasing, diacritics strippen (optioneel),\n",
    "# cijfers/punctuatie eruit, whitespace normaliseren.\n",
    "\n",
    "def simple_clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Eenvoudige normalisatie:\n",
    "    - lowercase\n",
    "    - verwijder niet-alfabetische karakters (bewust conservatief: a-z)\n",
    "    - collapse spaties\n",
    "\n",
    "    Let op:\n",
    "    - Afhankelijk van je corpus kun je liever accenten behouden of lemma's gebruiken.\n",
    "    - Hier kiezen we simpelheid, omdat de embeddings normaliter ook op plain tokens zijn getraind.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # vervang alles dat geen a-z is door spatie\n",
    "    text = re.sub(r\"[^a-z]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenizer na cleaning: split op spaties.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    return text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6036f322-68a7-41ad-a55f-08a3dfb42155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vec(\n",
    "    sentence: str,\n",
    "    embeddings: Dict[str, np.ndarray],\n",
    "    cleaner=simple_clean,\n",
    "    agg: str = \"sum\",   # \"sum\" of \"mean\"\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Zet een zin om naar één vector door embeddings van bekende tokens te aggregeren.\n",
    "    - Standaard gebruiken we de SOM (zoals gevraagd), 'mean' is een optionele variant.\n",
    "    - Tokens zonder embedding worden genegeerd.\n",
    "    - Retourneert None als er geen enkel bekend token is.\n",
    "\n",
    "    Waarom SOM?\n",
    "    - Simpel, lineair en effectief: document = 'bag of vectors'.\n",
    "    - 'Mean' is soms stabieler bij lengteverschillen; hier blijven we bij SOM zoals in de opdracht.\n",
    "    \"\"\"\n",
    "    cleaned = cleaner(sentence)\n",
    "    toks = tokenize(cleaned)\n",
    "    vecs = [embeddings[t] for t in toks if t in embeddings]\n",
    "    if not vecs:\n",
    "        return None\n",
    "    M = np.vstack(vecs).astype(np.float32)  # (n_tokens, d)\n",
    "    if agg == \"sum\":\n",
    "        v = M.sum(axis=0)\n",
    "    elif agg == \"mean\":\n",
    "        v = M.mean(axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"agg moet 'sum' of 'mean' zijn.\")\n",
    "    return v.astype(np.float32)\n",
    "\n",
    "\n",
    "def build_document_matrix(\n",
    "    sentences: Sequence[str],\n",
    "    embeddings: Dict[str, np.ndarray],\n",
    "    cleaner=simple_clean,\n",
    "    agg: str = \"sum\",\n",
    "    l2_normalize: bool = True\n",
    ") -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"\n",
    "    Maakt een (N_used, d)-matrix met documentvectoren voor alle zinnen.\n",
    "    - Zinnen zonder bekende tokens worden overgeslagen.\n",
    "    - Houdt indexmapping bij: 'used_idx[i]' verwijst naar originele zin-index.\n",
    "\n",
    "    l2_normalize=True:\n",
    "    - We normaliseren elke documentvector naar lengte 1.\n",
    "    - Hierdoor wordt cosine-similarity later een simpele dot-plot.\n",
    "    \"\"\"\n",
    "    doc_vecs = []\n",
    "    used_idx = []\n",
    "\n",
    "    # Dimensie afleiden uit een voorbeeldembedding\n",
    "    # (neemt aan dat embeddings niet leeg zijn)\n",
    "    example = next(iter(embeddings.values()))\n",
    "    d = example.shape[0]\n",
    "\n",
    "    for i, s in enumerate(sentences):\n",
    "        v = sentence_to_vec(s, embeddings, cleaner=cleaner, agg=agg)\n",
    "        if v is None:\n",
    "            continue\n",
    "        doc_vecs.append(v.reshape(1, -1))\n",
    "        used_idx.append(i)\n",
    "\n",
    "    if not doc_vecs:\n",
    "        raise ValueError(\"Geen enkele zin leverde een documentvector op (check cleaning/embeddings).\")\n",
    "\n",
    "    M = np.vstack(doc_vecs).astype(np.float32)  # (N_used, d)\n",
    "\n",
    "    if l2_normalize:\n",
    "        norms = np.linalg.norm(M, axis=1, keepdims=True)\n",
    "        norms = np.maximum(norms, 1e-12)\n",
    "        M = M / norms\n",
    "\n",
    "    return M, used_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15736af1-09fe-44d3-a2ed-31073a60dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LSHConfig:\n",
    "    num_planes: int = 16   # aantal hypervlakken → lengte signature (16 bits = 65536 buckets)\n",
    "    random_state: int = 42 # voor reproduceerbaarheid\n",
    "\n",
    "\n",
    "class RandomHyperplaneLSH:\n",
    "    \"\"\"\n",
    "    Locality Sensitive Hashing m.b.v. random hyperplanes:\n",
    "    - Maak 'num_planes' willekeurige vectoren r_j in ℝ^d.\n",
    "    - Voor elke documentvector x: bit_j = 1 als x·r_j >= 0 anders 0.\n",
    "    - Bucket-ID = interpretatie van de bitstring als integer.\n",
    "    - In deze opdracht zoeken we *één* bucket (exacte match op signatuur).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d: int, cfg: LSHConfig = LSHConfig()):\n",
    "        self.cfg = cfg\n",
    "        rng = np.random.default_rng(cfg.random_state)\n",
    "        # random hyperplanes: trek uit N(0,1); elke rij is één hypervlak\n",
    "        self.R = rng.standard_normal(size=(cfg.num_planes, d)).astype(np.float32)\n",
    "\n",
    "    def signature_bits(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Bepaal de bits (0/1) voor vector x t.o.v. alle hyperplanes.\n",
    "        Aanname: x shape = (d,) of (1,d)\n",
    "        \"\"\"\n",
    "        x = x.reshape(-1).astype(np.float32)\n",
    "        # scores: (num_planes,)\n",
    "        scores = self.R @ x\n",
    "        bits = (scores >= 0.0).astype(np.uint8)\n",
    "        return bits\n",
    "\n",
    "    @staticmethod\n",
    "    def bits_to_bucket(bits: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Zet bits (0/1) om naar een integer bucket-ID (msb-first).\n",
    "        \"\"\"\n",
    "        b = 0\n",
    "        for bit in bits.tolist():\n",
    "            b = (b << 1) | int(bit)\n",
    "        return b\n",
    "\n",
    "\n",
    "class LSHTable:\n",
    "    \"\"\"\n",
    "    Eén LSH-table met random hyperplanes.\n",
    "    - index(doc_matrix): maakt buckets: bucket_id -> list(doc_index)\n",
    "    - query(q): geeft indices uit dezelfde bucket terug (of lege lijst)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, doc_matrix: np.ndarray, lsh: RandomHyperplaneLSH):\n",
    "        \"\"\"\n",
    "        doc_matrix: (N, d) — idealiter L2-genormaliseerd\n",
    "        \"\"\"\n",
    "        self.doc_matrix = doc_matrix\n",
    "        self.lsh = lsh\n",
    "        self.buckets = {}  # dict[int, list[int]]\n",
    "\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        Stop alle documenten in buckets op basis van hun signatuur.\n",
    "        \"\"\"\n",
    "        N, d = self.doc_matrix.shape\n",
    "        self.buckets.clear()\n",
    "        for i in range(N):\n",
    "            x = self.doc_matrix[i]\n",
    "            bits = self.lsh.signature_bits(x)\n",
    "            bid = self.lsh.bits_to_bucket(bits)\n",
    "            self.buckets.setdefault(bid, []).append(i)\n",
    "\n",
    "    def query_bucket(self, q_vec: np.ndarray) -> List[int]:\n",
    "        \"\"\"\n",
    "        Geef de lijst documentindices (rij-indexen in doc_matrix) terug\n",
    "        die in *dezelfde* bucket zitten als q_vec. (Approximate KNN)\n",
    "        \"\"\"\n",
    "        bits = self.lsh.signature_bits(q_vec)\n",
    "        bid = self.lsh.bits_to_bucket(bits)\n",
    "        return self.buckets.get(bid, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4343f5d-f3b7-4a00-8c56-312972eb0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_topn_in_bucket(\n",
    "    q_vec: np.ndarray,\n",
    "    doc_matrix: np.ndarray,\n",
    "    candidate_idx: Sequence[int],\n",
    "    top_n: int = 5\n",
    ") -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Cosine-ranking (dot-product) van query q_vec tegen alle kandidaten in 'candidate_idx'.\n",
    "    Aanname: doc_matrix en q_vec zijn L2-genormaliseerd → cosine = dot.\n",
    "    Retourneert lijst van (doc_row_index, score), gesorteerd aflopend.\n",
    "    \"\"\"\n",
    "    if len(candidate_idx) == 0:\n",
    "        return []\n",
    "\n",
    "    # (1,d) dot (k,d)^T → (1,k)\n",
    "    C = doc_matrix[np.asarray(candidate_idx)]\n",
    "    q = q_vec.reshape(1, -1).astype(np.float32)\n",
    "    scores = (q @ C.T).ravel()\n",
    "    order = np.argsort(-scores)[:top_n]\n",
    "    return [(int(candidate_idx[i]), float(scores[i])) for i in order]\n",
    "\n",
    "\n",
    "def build_query_vec(\n",
    "    query_text: str,\n",
    "    embeddings: Dict[str, np.ndarray],\n",
    "    cleaner=simple_clean,\n",
    "    agg: str = \"sum\",\n",
    "    l2_normalize: bool = True\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Bouw de queryvector op precies dezelfde manier als documentvectoren.\n",
    "    \"\"\"\n",
    "    q = sentence_to_vec(query_text, embeddings, cleaner=cleaner, agg=agg)\n",
    "    if q is None:\n",
    "        return None\n",
    "    if l2_normalize:\n",
    "        n = np.linalg.norm(q)\n",
    "        if n < 1e-12:\n",
    "            return None\n",
    "        q = q / n\n",
    "    return q.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5588ac9-56c8-4c1e-866a-7faaf0b56b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SearchIndex:\n",
    "    sentences: List[str]           # originele zinnen\n",
    "    used_idx: List[int]            # mapping: doc_matrix[i] hoort bij sentences[used_idx[i]]\n",
    "    doc_matrix: np.ndarray         # (N_used, d), L2-genormaliseerd\n",
    "    lsh_table: LSHTable\n",
    "    embeddings_dim: int\n",
    "\n",
    "\n",
    "def build_search_index(\n",
    "    sentences: Sequence[str],\n",
    "    embeddings: Dict[str, np.ndarray],\n",
    "    cleaner=simple_clean,\n",
    "    agg: str = \"sum\",\n",
    "    num_planes: int = 16,\n",
    "    random_state: int = 42,\n",
    ") -> SearchIndex:\n",
    "    \"\"\"\n",
    "    Bouwt:\n",
    "    - Documentmatrix (L2-genormaliseerd)\n",
    "    - LSH met 'num_planes' hyperplanes (bucket-onderverdeling)\n",
    "    - LSH indexering van documenten\n",
    "    Retourneert een SearchIndex die je kunt gebruiken om te zoeken.\n",
    "    \"\"\"\n",
    "    doc_matrix, used_idx = build_document_matrix(\n",
    "        sentences, embeddings, cleaner=cleaner, agg=agg, l2_normalize=True\n",
    "    )\n",
    "    d = doc_matrix.shape[1]\n",
    "    lsh = RandomHyperplaneLSH(d, LSHConfig(num_planes=num_planes, random_state=random_state))\n",
    "    table = LSHTable(doc_matrix, lsh)\n",
    "    table.index()\n",
    "\n",
    "    return SearchIndex(\n",
    "        sentences=list(sentences),\n",
    "        used_idx=used_idx,\n",
    "        doc_matrix=doc_matrix,\n",
    "        lsh_table=table,\n",
    "        embeddings_dim=d\n",
    "    )\n",
    "\n",
    "\n",
    "def approximate_knn_search(\n",
    "    index: SearchIndex,\n",
    "    query_text: str,\n",
    "    embeddings: Dict[str, np.ndarray],\n",
    "    top_n: int = 5,\n",
    "    cleaner=simple_clean,\n",
    "    agg: str = \"sum\",\n",
    "    fallback_fullscan: bool = True\n",
    ") -> List[Tuple[str, float, int]]:\n",
    "    \"\"\"\n",
    "    Voert de complete Approximate KNN uit:\n",
    "    1) Query → queryvector (zelfde cleaning & aggregatie als corpus).\n",
    "    2) LSH: haal kandidaten uit *dezelfde* bucket.\n",
    "    3) Cosine-ranking binnen de bucket → top_n.\n",
    "    4) Optionele fallback: als bucket leeg is, doe een *volle* scan (langzamer maar robuuster).\n",
    "\n",
    "    Retourneert lijst: [(zin_tekst, score, originele_zin_index), ...]\n",
    "    \"\"\"\n",
    "    q_vec = build_query_vec(query_text, embeddings, cleaner=cleaner, agg=agg, l2_normalize=True)\n",
    "    if q_vec is None:\n",
    "        return []\n",
    "\n",
    "    cand = index.lsh_table.query_bucket(q_vec)\n",
    "\n",
    "    if not cand and fallback_fullscan:\n",
    "        # Fallback: geen kandidaten — doe full scan (exacte KNN) als safety-net.\n",
    "        # => Dit breekt de \"niet alle docs doorzoeken\" eis *alleen* als de bucket leeg is.\n",
    "        #    Je kunt fallback uitzetten als je strikt 1-bucket zoekt.\n",
    "        cand = list(range(index.doc_matrix.shape[0]))\n",
    "\n",
    "    top = cosine_topn_in_bucket(q_vec, index.doc_matrix, cand, top_n=top_n)\n",
    "\n",
    "    results = []\n",
    "    for row_i, score in top:\n",
    "        orig_i = index.used_idx[row_i]\n",
    "        results.append((index.sentences[orig_i], score, orig_i))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d548e7d-8dc9-4543-8665-2a20cbaec862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
